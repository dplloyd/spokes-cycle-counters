---
title: "Exploring Active Travel Open Data"
author: "diarmuid.lloyd@gmail.com"
output: 
  html_document:
    toc: true
    toc_float: true
theme: flatly
---



```{r, setup}
library(httr)
library(jsonlite)
library(tidyverse)
library(hrbrthemes)

hrbrthemes::import_roboto_condensed()

```


This is a notebook for recording different ways of accessing, and early data exploration, of the [Active Travel Open Data portal](https://usmart.io/org/cyclingscotland/). 

# Live counter data using API

The last 1000 trips can be accessed using the API (you can control this using the `limit` query parameter). This will be good for creating apps, or where we just need most recent data. The following chunks summarise how to access it. A `status_code()` return of 200 means our request has succeeded.


## Example

In this example, let's fetch the last four weeks of counts.

```{r}

four_weeks_ago <- as.Date(Sys.time() )- 28

four_weeks <- seq.Date(from = four_weeks_ago, to = Sys.Date(),by="day")



```


Using `purrr`, query each of these dates using the API and save everything in a dataframe.

```{r}

data <- purrr::map_df(
  four_weeks,
  .f = function(.x) {
    daily_counts <-
      GET(
        "https://api.usmart.io/org/d1b773fa-d2bd-4830-b399-ecfd18e832f3/7aa487cd-3cd5-405b-850e-1e2ac317816c/latest/urql",
        query  = list("startTime" = .x)
      )
    
    jsonlite::fromJSON(rawToChar(daily_counts$content))
    
  }
)

data <- data %>% mutate(startTime = lubridate::as_datetime(startTime))

```

Here, we do a very simple line plot of the accessed data of all counters.

```{r}


data %>% group_by(location) %>% 
  ggplot(aes(x = startTime, count, colour = location)) +
  geom_line() +
  theme(legend.position = "none")

```
We can see that it's not quite live. Today's date is `r Sys.Date()`, while the latest in the API call here was `r max(data$startTime)`.

Here, we create a table of the total number of counts by station in the past day, arranged in descending order of total counts.

```{r}
data %>% group_by( location) %>% 
  summarise(four_week_total = sum(count)) %>% 
  arrange(desc(four_week_total)) %>% 
  DT::datatable()
  
```


# Full datasets

The full dataset can be downloaded for each of:

* Daily bicycle counts
* Hourly bicycle counts
* Just Eat bicycle hire counts

As you might expect, these are rather large datasets, ranging 100MB-200MB.

# Daily bicycle counts

First read the data.

```{r}

df_od <- read_csv("data/cycling-scotland-edinburgh-daily-counts.csv")

glimpse(df_od)

df_od %>% group_by(startTime) %>% summarise(count)

```

With a quick check of the data, by plotting the daily totals for all counters.

```{r}
df_od %>% 
  group_by(startTime) %>%
  summarise(count = sum(count)) %>% 
  ggplot(aes(x = startTime, y = count)) +
  geom_line()
```

# Adding useful variables


```{r}

# Probably weekend effects at play, so create a weekend flag.
 df_od <- df_od %>% 
  mutate(weekend = if_else(lubridate::wday(startTime) %in% c(1,7), TRUE, FALSE ) ,
         day = lubridate::wday(startTime))

# Week starting flag, to allow summing trips in a given week
 df_od <- df_od %>% 
  mutate(week_starting = lubridate::floor_date(startTime, unit = "week"))
```


# Checking the stations of interest

**On Road - SfP**

* 031 Dundee Street
* 035 A90 Deans Bridge
* 048 Crewe Rd South
* 038 Bruntsfield Pl (Southbound)

**On Road - Non SfP**

* 052 A8 Wester Coates

**Off Road - Non SfP**

* 015 Middle Meadow Walk


Manually construct a tribble which holds the stations of interest, with a SpF flag, then match that in to the main dataset.

```{r}
df_od %>% count(location,siteID)

site_id <- tribble(~siteID, ~sfp,
                    "EDH0035", "on-road SfP",
                    "EDH0031", "on-road SfP",
                   "EDH0048", "on-road SfP",
                   "EDH0039", "on-road SfP",
                   "EDH0052", "on-road non-SfP",
                   "EDH55500015", "off-road non-SfP") %>% 
  mutate(sfp = as.factor(sfp))


df_od <- left_join(df_od,site_id, by = "siteID")

```

We now filter the data for those stations, and plot.

```{r}
df_f <- df_od %>% filter( siteID %in% site_id$siteID ) %>% 
  mutate(year =lubridate::year(startTime) )

df_f %>% 
  ggplot() +
  geom_line(aes(x = startTime, y = count, colour= year)) +
  facet_wrap(~location)


```


Weekly totals, ignoring any dodgy data, etc.


```{r}

df_f %>% group_by(siteID,location,sfp,week_starting) %>% 
  summarise(count = sum(count, na.rm = TRUE)) %>% 
  ggplot() +
    geom_line(aes(x = week_starting, y = count, colour = sfp)) +
  facet_wrap(~location) +
  
  labs(title = "Weekly bicycle counts for selected stations",
       subtitle = "No data cleaning")

ggsave("output/weekly_bike_counts_selected_stations.png", plot = last_plot())  



```

The distribution of total daily counts per year might be more helpful. Here, we compare 2019 and 2021:


```{r}

bin_width = 50

df_f %>% group_by(siteID,location,year,week_starting, day) %>% 
  filter(count > 0,
         year ==2019 | year==2021) %>% 
  summarise(count = sum(count, na.rm = TRUE)) %>% 
  
  
  ggplot(aes(x = count, group = year, fill = as.factor(year))) +
  geom_histogram(
    aes(y = stat(density*width)),
    alpha = 0.5, position = "identity") +
  facet_wrap(~location, scales = "free") +
  scale_x_continuous(name = "Daily count") 

ggsave("output/daily_counts_2019_2020_histogram_selected_stations.png", plot = last_plot())  


```



