---
title: "Building Open Travel Databases"
output: html_document
author: "Diarmuid Lloyd (diarmuid.lloyd@gmail.com)"
date: "First written: October 2021. Last run: `r Sys.Date()`"
editor_options: 
  chunk_output_type: console
---

# Purpose

The purpose of this script is to build a consolidated dataset consisting of both ECC bicycle counter data, and Just Eats trip data. The ECC data were originally provided in a series `csv` files obtained via a FOI request, but subsequently I learned near-live ECC data are published on the [Cycling Scotland Open Data portal](https://usmart.io/org/cyclingscotland/discovery?order=_score&keywords=edinburgh&limit=20&offset=0). As the FOI data stretches back further, here we treat the `csv` files separately, then match in the latest data.

The Just Eat bicyle rental scheme stopped in September 2021. The full data set of trips are also accessible from [Cycling Scotland Open Data portal](https://usmart.io/org/cyclingscotland/discovery?order=_score&keywords=edinburgh&limit=20&offset=0).

<hr>


# ECC Counter data

JR provided data from an FOI request. ECC supplied the the data as `csv` files, organised in sub folders for each counter location. In total, there's around 500MB of data. Further, the files are prefixed with either `bin_` or `pvr_`, with a small amount of overlap. The approach here is to loop over all `bin_` and `pvr_` files separately, creating two tables, before merging into a final table.


First, set out packages. Note, I've set glbal evaluation to `FALSE` as this document takes ages to knit otherwise. Either run each chunk manually, or delete `knitr::opts_chunk$set(eval = FALSE)`.
```{r, setup}

## Packages
library(tidyverse)
library(dbplyr)
library(DBI)

#Global options
knitr::opts_chunk$set(eval = FALSE)


```


Create the database which we'll store our data:
```{r}
ecc_counter_db_file <- "data/ecc-counter-database-output.sqlite"
ecc_counter_db <- DBI::dbConnect(drv = RSQLite::SQLite(), dbname =  ecc_counter_db_file)
```


Define a function which searches across all the folders and consolidated the data into the database defined above. 
```{r}

# Function reads in data from across folders and writes to table.
# Table distinguished by the leading prefix of filename, which must be supplied.
# prefix  =   pvr or bin
# ecc_counter_db = ecc_counter_db defined above
write_cycle_count_db <- function(prefix, ecc_counter_db){
  
  # Gets all the sub-dir names containing csv files which start with bin_.
  list_of_dirs<-
    dir("data/", full.names = TRUE, recursive = TRUE) %>%
    str_subset(".csv") %>%
    str_subset(paste0(prefix,"_")) 
  
  # separates out some identifying information on each counter station from filenames and paths
  counters <- tibble(path = dir("data/", recursive = TRUE) %>%
                       str_subset(".csv") %>%
                       str_subset(paste0(prefix,"_"))) %>% 
    separate(col = path, into = c("station_name","filename"), sep = "/", remove = FALSE) %>% 
    separate(col= station_name, c("station_num","station_name"), sep = "\\s", extra = "merge")
  
  # Carry out some checks of counters
  counters %>% count(station_name) 
  
  
  # Deal with the bin files first
  #do as a loop, as purrr resulting in memory issues
  first_run <- TRUE
  for (i in 1:length(list_of_dirs)) {
    print(list_of_dirs[i])
    #files don't have constant header and fields, so don't read colnames
    df <- read.table(list_of_dirs[i],sep=",", fill = TRUE, header = FALSE) 
    
    #Now assign colnames, and delete first row which held the names
    colnames(df) = df[1,]
    df <- df[-1,]
    
    # Choose the fields of interest.
    df <- df %>%
      select(
        date = Sdate,
        direction_description = DirectionDescription,
        volume = Volume,
        flag = Flags,
        flag_text = `Flag Text`
      ) %>%
      type_convert(col_types = cols(date=col_datetime())) %>% 
      mutate(#path = counters$path[i],
        station_num = counters$station_num[i],
        station_name = counters$station_name[i],
        filename = counters$filename[i])
    
    if (first_run == TRUE) {
      dbWriteTable(ecc_counter_db, paste0("station_counts_hour_",prefix), df, overwrite = TRUE)
      first_run = FALSE
    }
    else {
      dbWriteTable(ecc_counter_db, paste0("station_counts_hour_",prefix), df, append = TRUE)
    }  
    
    rm(df)
    
    
  }
  
}

```

We now use this function to consolidate the two flavours of `csv` file to the database:

```{r}

write_cycle_count_db("bin",  ecc_counter_db)
write_cycle_count_db("pvr",  ecc_counter_db)
dbDisconnect(ecc_counter_db)

```


## Merging bin and pvr

Now we must determine the overlap between each of these two datasets, and merge in a final consolidated file.


```{r}

# Load the data

con <- DBI::dbConnect(RSQLite::SQLite(), dbname = "data/ecc-counter-database-output.sqlite")

df_bin <- tbl(con, "station_counts_hour_bin") %>% collect() %>%  
  mutate(date = lubridate::as_datetime(date))

df_pvr <- tbl(con, "station_counts_hour_pvr") %>% collect() %>% 
  mutate(date = lubridate::as_datetime(date))


```


Compare the structure of each dataframe

```{r}
glimpse(df_bin)
glimpse(df_pvr)
```

They have the same structure. For each station, we want to know the time overlap. Note that time here is current in second relative to a fixed point, so I will convert to datetime class later.

```{r}

df_combo <- rbind(df_bin,df_pvr) 

# Write the joined version to DB.
dbWriteTable(con,"station_counts_hour_joined", df_combo, overwrite = TRUE)

      
#Disconnect from db
dbDisconnect(con)

```

Now attempting to ID where there are duplicates and apply a sensible way of picking either the pvr or bin files.

```{r}

df_combo <- df_combo %>% 
  group_by(station_num, date, direction_description) %>% 
  arrange(by_group = TRUE) 

#Expect groups to have a size of 2
groups_with_issues = df_combo %>% 
  filter(n() >=3 ) 


```


There are a small number of duplicate records between the `bin` and `pvr` files. Not all of the volume counts agree, either. Further, there are instances of more than two records for a given time period for the same station, in the same direction. I would have assumed only two would be expected. Here's an example:


```{r}
groups_with_issues 
```


The covernote with the data states:

> CEC Active Travel counter historic data up to April 2021 (Raw Data)
  The data has been bundled per counter per year in bins of 1 hour.
The transfer to the new website took place in 2019 and due to formatting, some counters have two spreadsheets for this year (one with data pretransfer and one with data post transfer).
 Those counters that have the capability to differentiate between different group users (i.e.  pedestrians, bike users, scooters and motorbikes) have a dedicated column with the breakdown, however, the raw data only shows this as a different type of “vehicle”. 
Data may be inaccurate due to maintenance, communication issues, road works, and/or misuse by users. 

So I'm not sure how to interpret the disagreements in the context of this note. Something to bear in mind for the data cleaning, however. In terms of the distribution of the number of counts per station, per time period, per direction, we would expect two. So could either take mean, or chuck the duplicates.









