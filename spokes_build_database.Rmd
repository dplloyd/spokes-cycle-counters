---
title: "Building Open Travel Databases"
output: html_notebook
author: "Diarmuid Lloyd (diarmuid.lloyd@gmail.com)"
date: "First written: October 2021. Last run: `r Sys.Date()`"
---

# Purpose

The purpose of this script is to build a consolidated dataset consisting of both ECC bicycle counter data, and Just Eats trip data. The ECC data were originally provided in a series `csv` files obtained via a FOI request, but subsequently I learned near-live ECC data are published on the [Cycling Scotland Open Data portal](https://usmart.io/org/cyclingscotland/discovery?order=_score&keywords=edinburgh&limit=20&offset=0). As the FOI data stretches back further, here we treat the `csv` files separately, then match in the latest data.

The Just Eat bicyle rental scheme stopped in September 2021. The full data set of trips are also accessible from [Cycling Scotland Open Data portal](https://usmart.io/org/cyclingscotland/discovery?order=_score&keywords=edinburgh&limit=20&offset=0).

<hr>


# ECC Counter data

JR provided data from an FOI request. ECC supplied the the data as `csv` files, organised in sub folders for each counter location. In total, there's around 500MB of data. Further, the files are prefixed with either `bin_` or `pvr_`, with a small amount of overlap. The approach here is to loop over all `bin_` and `pvr_` files separately, creating two tables, before merging into a final table.


First, set out packages:
```{r}

## Packages
library(tidyverse)
library(dbplyr)
library(DBI)
```


Create the database which we'll store our data:
```{r}
ecc_counter_db_file <- "data/ecc-counter-database-output.sqlite"
ecc_counter_db <- DBI::dbConnect(drv = RSQLite::SQLite(), dbname =  ecc_counter_db_file)
```


Define a function which searches across all the folders and consolidated the data into the database defined above. 
```{r}

# Function reads in data from across folders and writes to table.
# Table distinguished by the leading prefix of filename, which must be supplied.
# prefix  =   pvr or bin
# ecc_counter_db = ecc_counter_db defined above
write_cycle_count_db <- function(prefix, ecc_counter_db){
  
  # Gets all the sub-dir names containing csv files which start with bin_.
  list_of_dirs<-
    dir("data/", full.names = TRUE, recursive = TRUE) %>%
    str_subset(".csv") %>%
    str_subset(paste0(prefix,"_")) 
  
  # separates out some identifying information on each counter station from filenames and paths
  counters <- tibble(path = dir("data/", recursive = TRUE) %>%
                       str_subset(".csv") %>%
                       str_subset(paste0(prefix,"_"))) %>% 
    separate(col = path, into = c("station_name","filename"), sep = "/", remove = FALSE) %>% 
    separate(col= station_name, c("station_num","station_name"), sep = "\\s", extra = "merge")
  
  # Carry out some checks of counters
  counters %>% count(station_name) 
  
  
  # Deal with the bin files first
  #do as a loop, as purrr resulting in memory issues
  first_run <- TRUE
  for (i in 1:length(list_of_dirs)) {
    print(list_of_dirs[i])
    #files don't have constant header and fields, so don't read colnames
    df <- read.table(list_of_dirs[i],sep=",", fill = TRUE, header = FALSE) 
    
    #Now assign colnames, and delete first row which held the names
    colnames(df) = df[1,]
    df <- df[-1,]
    
    # Choose the fields of interest.
    df <- df %>%
      select(
        date = Sdate,
        direction_description = DirectionDescription,
        volume = Volume,
        flag = Flags,
        flag_text = `Flag Text`
      ) %>%
      type_convert(col_types = cols(date=col_datetime())) %>% 
      mutate(#path = counters$path[i],
        station_num = counters$station_num[i],
        station_name = counters$station_name[i],
        filename = counters$filename[i])
    
    if (first_run == TRUE) {
      dbWriteTable(ecc_counter_db, paste0("station_counts_hour_",prefix), df, overwrite = TRUE)
      first_run = FALSE
    }
    else {
      dbWriteTable(ecc_counter_db, paste0("station_counts_hour_",prefix), df, append = TRUE)
    }  
    
    rm(df)
    
    
  }
  
}

```

We now use this function to consolidate the two flavours of `csv` file to the database:

```{r}

write_cycle_count_db("bin",  ecc_counter_db)
write_cycle_count_db("pvr",  ecc_counter_db)
dbDisconnect(ecc_counter_db)

```


## Merging bin and pvr

Now we must determine the overlap between each of these two datasets, and merge in a final consolidated file.


```{r}

# Load the data

con <- DBI::dbConnect(RSQLite::SQLite(), dbname = "data/ecc-counter-database-output.sqlite")

df_bin <- tbl(con, "station_counts_hour_bin") %>% collect()

df_pvr <- tbl(con, "station_counts_hour_pvr") %>% collect()

dbDisconnect(con)


# df_bin <- df_bin %>% group_by(date, direction_description)
# df_pvr <- df_pvr %>% group_by(date, direction_description)

```


Compare the structure of each dataframe

```{r}
glimpse(df_bin)
glimpse(df_pvr)
```

They have the same structure. For each station, we want to know the time overlap. Note that time here is current in second relative to a fixed point, so I will convert to datetime class later.

```{r}

# df_bin <- df_bin %>% group_by(station_num, direction_description)
# 
# df_pvr <- df_pvr %>% group_by(station_num, direction_description)

df_combo <- full_join(df_bin,df_pvr,
                      by = c("station_name","direction_description","date"), keep = TRUE)

df_combo_same <- df_combo %>%  
  dplyr::filter(!is.na(volume.y) & !is.na(volume.x) & (volume.x == volume.y )) %>% nrow()

df_combo_duplicate <- df_combo %>%  
  dplyr::filter(!is.na(volume.y) & !is.na(volume.x) & (volume.x != volume.y ))

```

Variable `df_combo_duplicate` holds the station records from bin and pvr file variants that do not agree with each other. In total, `r scales::percent(nrow(df_combo_duplicate)/nrow(df_combo), accuracy=0.01)` of the records have duplicate time-stamp in pvr files and do not agree.


# Inspecting the data sets


We use `GGally` and `visdat` which have some useful functions.
